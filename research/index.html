<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Research | Chuanke Pang</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Research" />
<meta property="og:description" content="Research Interests My Research mainly focuses on Artificial Intelligence of Things (AIoT), where the state-of-the-art AI models are proposed and implemented in the edge devices and enables everything interconnected. Broadly speaking, I study how AI models empower IoT-enabled edge devices to sense the environment (i.e. AIoT Sensing), how AI models are efficiently deployed in resource-restrained environments (i.e. Efficient AI), how multi-modal IoT data can be represented to support decision-making in large language models (i." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://marsyang.site/research/" /><meta property="article:section" content="" />




	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	<link rel="stylesheet" href="/css/mystyle.css">

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="Chuanke Pang (Kris)" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">Chuanke Pang (Kris)</div>
					<div class="logo__tagline">Science is the only way to magic.</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/">
				
				<span class="menu__text">Home</span>
				
			</a>
		</li>
		<li class="menu__item menu__item--active">
			<a class="menu__link" href="/research/">
				
				<span class="menu__text">Research</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/publications/">
				
				<span class="menu__text">Publications</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/patents/">
				
				<span class="menu__text">Patents</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/award/">
				
				<span class="menu__text">Awards</span>
				
			</a>
		</li>
		<!--<li class="menu__item">
			<a class="menu__link" href="/group/">
				
				<span class="menu__text">Group</span>
				
			</a>
		</li>-->
		<li class="menu__item">
			<a class="menu__link" href="/services/">
				
				<span class="menu__text">Services</span>
				
			</a>
		</li>
		<!--<li class="menu__item">
			<a class="menu__link" href="/friends/">
				
				<span class="menu__text">Friends</span>
				
			</a>
		</li>-->
	</ul>
</nav>
	</div>
</header>


		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Research</h1>
			
		</header>
		<div class="content post__content clearfix">
			<hr>
<h3 id="research-interests">Research Interests</h3>
<p>My Research mainly focuses on <strong>Artificial Intelligence of Things (AIoT)</strong>, where the state-of-the-art AI models are proposed and implemented in the edge devices and enables everything interconnected. Broadly speaking, I study how AI models empower IoT-enabled edge devices to sense the environment (i.e. <strong>AIoT Sensing</strong>), how AI models are efficiently deployed in resource-restrained environments (i.e. <strong>Efficient AI</strong>), how multi-modal IoT data can be represented to support decision-making in large language models (i.e. <strong>Multimodal LLM</strong>).</p>
<p>Revolving these goals, my team mainly studies the following topics:</p>
<ul>
<li><strong>Deep Learning Empowered Wireless AIoT Sensing</strong>: Computer vision has been very powerful but it is still limited in real-world scenarios regarding the illumination, occlusion and privacy issue. Wireless AIoT sensing leverages non-intrusive sensors, e.g., mmWave radar, WiFi signals, and ultra-wideband (UWB), for human perception. Handling these IoT data is more challenging due to the lack of understanding of various sensor data. We develop deep learning, transfer learning, few-shot learning and self-supervised learning algorithms to enable cost-effective, data-efficient, privacy-preserving and fine-grained wireless sensing technology.</li>
<li><strong>Efficient AI</strong>: Deep networks help AI systems achieve remarkable performances but they are limited by the requirement of massive labeled data and powerful computational devices. In AIoT sensing, AI models should be simple enough to run in the real-time systems, which denotes our objective. We develop efficient AI models on real-time systems using edge devices, such as Raspberry Pi and NVIDIA Jetson Nano. Based on them, everything can be interconnected.</li>
<li><strong>Multimodal LLM</strong>: LLM has revolutionized the world with the capability of zero-shot tasks. Currently the LLM only recognizes texts and images. My team aims to enable LLM to connect IoT data and get deeply involved in IoT-enabled systems, e.g., robotics and smart manufacturing.</li>
</ul>
<hr>

<h3 id="research-experience">Research Experience</h3>
<div class="list-item publication" data-category="publication">
	<a href="https://video-language-planning.github.io/" class="thumbnail">
	  <img src="https://video-language-planning.github.io/img/share_image.png" alt="" />
	</a>
	<div class="project-description">
	  <h3><a href="https://video-language-planning.github.io/">Video Language Planning</a></h3>
	  <p>
		Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum, Leslie Kaelbling, Andy Zeng, Jonathan Tompson<br>
		  <!-- <i>Conference on Robot Learning (CoRL) 2023</i><br> -->
		  <a href="https://video-language-planning.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://arxiv.org/abs/2310.10625">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://github.com/video-language-planning/vlp_code">Code</a>
		  <br> <!-- TODO: extra spaces until I figure out the margin bug. -->
	  </p>
	</div>
  </div>

  <div class="list-item publication" data-category="publication">
	<a href="https://general-pattern-machines.github.io/" class="thumbnail">
		<a href="https://video-language-planning.github.io/" class="thumbnail">
			<img src="https://video-language-planning.github.io/img/Astronaut-Robot Collaborative Manipulation.png" alt=""/>
		</a>
	</a>
	<div class="project-description">
	  <h3><a href="https://general-pattern-machines.github.io/">Large Language Models as General Pattern Machines</a></h3>
	  <p>
		Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, Andy Zeng<br>
		  <i>Conference on Robot Learning (CoRL) 2023</i><br>
		  <a href="https://general-pattern-machines.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://arxiv.org/abs/2307.04721">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://general-pattern-machines.github.io/#colabs">Code</a>
	  </p>
	</div>
  </div>

  <div class="list-item publication" data-category="publication">
	<a href="https://robot-help.github.io/" class="thumbnail">
	  <video playsinline="" muted="" autoplay="" loop="" width="180px">
		<source src="https://chuankepang.github.io/image/Dexterous_manipulation.png" type="video/mp4">
	  </video>
	</a>
	<div class="project-description">
	  <h3><a href="https://robot-help.github.io/">Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners</a></h3>
	  <p>
		Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, Anirudha Majumdar<br>
		  <i>Conference on Robot Learning (CoRL) 2023</i><br>
		  <font color="49bf9"><i>&#9733; Oral Presentation, Best Student Paper Award, CoRL &#9733;</i></font><br>
		  <a href="https://robot-help.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://arxiv.org/abs/2307.01928">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://youtu.be/xCXx09gfhx4">Video</a>
	  </p>
	</div>
  </div>

  <div class="list-item publication" data-category="publication">
	<a href="https://language-to-reward.github.io/" class="thumbnail">
	  <video playsinline="" muted="" autoplay="" loop="" width="180px">
		<source src="https://chuankepang.github.io/image/Astronaut-Robot Collaborative Manipulation.png" type="video/mp4">
	  </video>
	</a>
	<div class="project-description">
		<p>&nbsp;</p>
		<p>&nbsp;</p>
		<p>&nbsp;</p>
		<p>&nbsp;</p>
	  <h3><a href="https://language-to-reward.github.io/">Language to Rewards for Robotic Skill Synthesis</a></h3>
	  <p>
		 m Erez, Leonard Hasenclever, Jan Humplik, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie Tan, Yuval Tassa, Fei Xia<br>
		  <i>Conference on Robot Learning (CoRL) 2023</i><br>
		  <font color="49bf9"><i>&#9733; Oral Presentation, CoRL &#9733;</i></font><br>
		  <a href="https://language-to-reward.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://arxiv.org/abs/2306.08647">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://youtu.be/7KiKg0rdSSQ">Video</a>
	  </p>
	</div>
  </div>

  <div class="list-item publication" data-category="publication">
	<a href="https://general-part-assembly.github.io/" class="thumbnail">
	  <img src="https://chuankepang.github.io/image/Dexterous_manipulation.png" alt="" />
	</a>
	<div class="project-description">
	  <h3><a href="https://general-part-assembly.github.io/">Rearrangement Planning for General Part Assembly</a></h3>
	  <p>
		Yulong Li, Andy Zeng, Shuran Song<br>
		  <i>Conference on Robot Learning (CoRL) 2023</i><br>
		  <font color="49bf9"><i>&#9733; Oral Presentation, CoRL &#9733;</i></font><br>
		  <a href="https://general-part-assembly.github.io/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://arxiv.org/abs/2307.00206">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://general-part-assembly.github.io/figs/video.mp4">Video</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://github.com/columbia-ai-robotics/gpat/">Code</a>
	  </p>
	</div>
  </div>

  <div class="list-item publication" data-category="publication">
	<a href="https://openreview.net/forum?id=Nii0_rRJwN" class="thumbnail">
	  <img src="images/calamari-teaser.png" alt="" />
	</a>
	<div class="project-description">
	  <h3><a href="https://openreview.net/forum?id=Nii0_rRJwN">CALAMARI: Contact-Aware and Language conditioned spatial Action MApping for contact-RIch manipulation</a></h3>
	  <p>
		Youngsun Wi, Mark Van der Merwe, Pete Florence, Andy Zeng, Nima Fazeli<br>
		  <i>Conference on Robot Learning (CoRL) 2023</i><br>
		  <a href="https://openreview.net/pdf?id=Nii0_rRJwN">PDF</a>
	  </p>
	</div>
  </div>

  <div class="list-item publication" data-category="publication">
	<a href="https://tidybot.cs.princeton.edu/" class="thumbnail">
	  <video playsinline="" muted="" autoplay="" loop="" width="180px">
		<source src="images/tidybot-teaser.mp4" type="video/mp4">
	  </video>
	</a>
	<div class="project-description">
	  <h3><a href="https://tidybot.cs.princeton.edu/">TidyBot: Personalized Robot Assistance with Large Language Models</a></h3>
	  <p>
		Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, Thomas Funkhouser<br>
		  <i>IEEE International Conference on Intelligent Robots and Systems (IROS) 2023</i><br>
		  <i>Autonomous Robots 2023</i><br>
		  <a href="https://tidybot.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://arxiv.org/pdf/2305.05658.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://github.com/jimmyyhwu/tidybot">Code</a>
		  <br> <!-- TODO: extra spaces until I figure out the margin bug. -->
	  </p>
	</div>
  </div>

  <div class="list-item publication" data-category="publication">
	<a href="https://arxiv.org/abs/2306.05392" class="thumbnail">
	  <video playsinline="" muted="" autoplay="" loop="" width="180px">
		<source src="images/codevqa-teaser...........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................mp4" type="video/mp4">
	  </video>
	</a>
	<div class="project-description">
	  <h3><a href="https://arxiv.org/abs/2306.05392">Modular Visual Question Answering via Code Generation</a></h3>
	  <p>
		Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, Dan Klein<br>
		  <i>Association for Computational Linguistics (ACL) 2023</i><br>
		  <a href="https://arxiv.org/pdf/2306.05392.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://github.com/sanjayss34/codevqa">Code</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
		  <a href="https://ai.googleblog.com/2023/07/modular-visual-question-answering-via.html">Google AI Blog</a>
		  <br> <!-- TODO: extra spaces until I figure out the margin bug. -->
	  </p>
	</div>
  </div>
<p>&nbsp;</p>
<hr>
<h3 id="collaborations">Collaborations</h3>
<p>I am interested in collaboration with respect to the following directions:</p>
<ul>
<li>IoT-enabled Human Perception and Its Applications (Smart Home and Robotics)</li>
<li>Affective Computing via Computer Vision and IoT Sensors (Human-Centric AI Systems)</li>
<li>Deep Learning and Transfer Learning Algorithms and Applications for Interdisciplinary Research (Biology and Medicine)</li>
</ul>



		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 Chuanke Pang (Kris).
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>
